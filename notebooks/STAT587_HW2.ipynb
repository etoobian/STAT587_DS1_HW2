{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2623dba2",
   "metadata": {},
   "source": [
    "# **STAT 587 &mdash; Data Science I** &mdash; Homework 2\n",
    "**Winter 2026**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09857722",
   "metadata": {},
   "source": [
    "## Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb29fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.append(str(PROJECT_ROOT))\n",
    "\n",
    "# Ensure output directories exist\n",
    "from src.paths import ensure_dirs, DATA_DIR, FIG_DIR, TAB_DIR\n",
    "\n",
    "ensure_dirs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8155f99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ISLP import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68425fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9841a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL OUTPUT CONTROLS\n",
    "\n",
    "VERBOSE = True           # print / display intermediate outputs\n",
    "SAVE_FIGS = True         # write figures to reports/figures\n",
    "SAVE_TABLES = True       # write tables to reports/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb591c",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f944bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG HELPERS\n",
    "\n",
    "\n",
    "# OUTPUT\n",
    "def vprint(*args, **kwargs):\n",
    "    if VERBOSE:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "\n",
    "# FIGURES\n",
    "def save_fig(fig, filename, *, dpi=300):\n",
    "    \"\"\"Optionally save matplotlib figure to reports/figures and close it.\"\"\"\n",
    "    if SAVE_FIGS:\n",
    "        path = FIG_DIR / filename\n",
    "        fig.savefig(path, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "        return path\n",
    "    plt.close(fig)\n",
    "    return None\n",
    "\n",
    "\n",
    "# LaTex TABLES\n",
    "def latex_escape_underscores(s):\n",
    "    \"\"\"Escape underscores for LaTeX.\"\"\"\n",
    "    return str(s).replace(\"_\", r\"\\_\")\n",
    "\n",
    "\n",
    "def escape_df_underscores(df, cols=(\"Predictor\",)):\n",
    "    \"\"\"Escape underscores in specified string columns (returns a copy).\"\"\"\n",
    "    out = df.copy()\n",
    "    for c in cols:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(str).apply(latex_escape_underscores)\n",
    "    return out\n",
    "\n",
    "\n",
    "def df_to_tabular_tex(df, *, float_fmt=\"%.4f\", index=True):\n",
    "    \"\"\"Return LaTeX tabular with booktabs formatting.\"\"\"\n",
    "    return df.to_latex(\n",
    "        index=index,\n",
    "        escape=False,\n",
    "        float_format=(lambda x: float_fmt % x) if float_fmt else None,\n",
    "        bold_rows=False,\n",
    "        longtable=False,\n",
    "    )\n",
    "\n",
    "\n",
    "def wrap_table(tabular_tex, *, caption, label):\n",
    "    \"\"\"Wrap a tabular in a standalone LaTeX table environment.\"\"\"\n",
    "    return \"\\n\".join(\n",
    "        [\n",
    "            r\"\\begin{table}[H]\",\n",
    "            r\"\\begin{center}\",\n",
    "            tabular_tex.strip(),\n",
    "            r\"\\end{center}\",\n",
    "            r\"\\vspace{-5pt}\",\n",
    "            rf\"\\caption{{{caption}}}\",\n",
    "            rf\"\\label{{{label}}}\",\n",
    "            r\"\\end{table}\",\n",
    "            \"\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def save_tex_table(\n",
    "    df,\n",
    "    filename,\n",
    "    *,\n",
    "    caption,\n",
    "    label,\n",
    "    float_fmt=\"%.4f\",\n",
    "    index=False,\n",
    "    escape_underscore_cols=None,\n",
    "):\n",
    "    \"\"\"Optionally save a DataFrame as a LaTeX table to reports/tables.\"\"\"\n",
    "    if not SAVE_TABLES:\n",
    "        return None\n",
    "\n",
    "    if escape_underscore_cols:\n",
    "        df = escape_df_underscores(df, cols=escape_underscore_cols)\n",
    "\n",
    "    tex = wrap_table(\n",
    "        df_to_tabular_tex(df, float_fmt=float_fmt, index=index),\n",
    "        caption=caption,\n",
    "        label=label,\n",
    "    )\n",
    "    path = TAB_DIR / filename\n",
    "    path.write_text(tex)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad135b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2da0de",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca9716",
   "metadata": {},
   "source": [
    "## **Question 1:** College Data (ISLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e01b4",
   "metadata": {},
   "source": [
    "### Data Load, Initial Inspection, Split, Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "q1_df = load_data(\"College\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d4a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial EDA summary\n",
    "\n",
    "# Identify categorical vs numeric columns\n",
    "q1_cat_cols = q1_df.select_dtypes(\n",
    "    include=[\"object\", \"category\", \"bool\"]\n",
    ").columns.tolist()\n",
    "q1_num_cols = [c for c in q1_df.columns if c not in q1_cat_cols]\n",
    "\n",
    "# Categorical Variable Inspection\n",
    "q1_cat_levels = {}\n",
    "for col in q1_cat_cols:\n",
    "    levels = sorted(q1_df[col].astype(str).unique().tolist())\n",
    "    q1_cat_levels[col] = levels\n",
    "\n",
    "q1_cat_levels_str = (\n",
    "    \"; \".join([f\"{col}:\" + \"|\".join(levels) for col, levels in q1_cat_levels.items()])\n",
    "    if q1_cat_levels\n",
    "    else \"(none)\"\n",
    ")\n",
    "\n",
    "# Numeric Variable Inspection\n",
    "q1_num_summary = q1_df[q1_num_cols].describe().T\n",
    "\n",
    "q1_num_ranges_str = (\n",
    "    \"; \".join(\n",
    "        [\n",
    "            f\"{col}[{q1_num_summary.loc[col, 'min']:.0f},{q1_num_summary.loc[col, 'max']:.0f}]\"\n",
    "            for col in q1_num_summary.index\n",
    "        ]\n",
    "    )\n",
    "    if len(q1_num_cols) > 0\n",
    "    else \"(none)\"\n",
    ")\n",
    "\n",
    "# Target variable inspection\n",
    "q1_apps_summary = q1_df[\"Apps\"].describe()\n",
    "\n",
    "q1_apps_str = (\n",
    "    f\"min={q1_apps_summary['min']:.0f}, \"\n",
    "    f\"q1={q1_apps_summary['25%']:.0f}, \"\n",
    "    f\"median={q1_apps_summary['50%']:.0f}, \"\n",
    "    f\"mean={q1_apps_summary['mean']:.1f}, \"\n",
    "    f\"q3={q1_apps_summary['75%']:.0f}, \"\n",
    "    f\"max={q1_apps_summary['max']:.0f}\"\n",
    ")\n",
    "\n",
    "# Build EDA  Summary table\n",
    "q1_eda_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"item\": [\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "            \"columns\",\n",
    "            \"categorical_cols\",\n",
    "            \"categorical_levels\",\n",
    "            \"numeric_cols\",\n",
    "            \"numeric_ranges_minmax\",\n",
    "            \"target\",\n",
    "            \"Apps_summary\",\n",
    "        ],\n",
    "        \"value\": [\n",
    "            q1_df.shape[0],\n",
    "            q1_df.shape[1],\n",
    "            \", \".join(q1_df.columns.astype(str).tolist()),\n",
    "            \", \".join(q1_cat_cols) if q1_cat_cols else \"(none)\",\n",
    "            q1_cat_levels_str,\n",
    "            \", \".join(q1_num_cols),\n",
    "            q1_num_ranges_str,\n",
    "            \"Apps\",\n",
    "            q1_apps_str,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# q1_eda_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4039b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Checks\n",
    "assert len(q1_df) > 0, \"College dataset is empty.\"\n",
    "assert \"Apps\" in q1_df.columns, \"Target variable 'Apps' not found.\"\n",
    "assert q1_df.isna().sum().sum() == 0, \"Unexpected missing values in College dataset.\"\n",
    "\n",
    "# Categorical checks\n",
    "if \"Private\" in q1_df.columns:\n",
    "    assert set(q1_df[\"Private\"].astype(str).unique()) == {\n",
    "        \"Yes\",\n",
    "        \"No\",\n",
    "    }, \"Unexpected levels in 'Private' variable.\"\n",
    "\n",
    "# Numeric checks\n",
    "assert (\n",
    "    q1_df[q1_num_cols].isna().sum().sum() == 0\n",
    "), \"Missing values in numeric predictors.\"\n",
    "assert all(\n",
    "    pd.api.types.is_numeric_dtype(q1_df[c]) for c in q1_num_cols\n",
    "), \"Non-numeric dtype found in numeric predictors.\"\n",
    "\n",
    "# Target checks\n",
    "assert (q1_df[\"Apps\"] > 0).all(), \"Applications count must be positive.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c5d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing data\n",
    "n_missing = int(q1_df.isna().sum().sum())\n",
    "assert n_missing == 0, f\"Missing values exist in data: total missing = {n_missing}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ef4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20)\n",
    "q1_X = q1_df.drop(columns=[\"Apps\"])\n",
    "q1_y = q1_df[\"Apps\"].to_numpy()\n",
    "\n",
    "q1_X_train, q1_X_test, q1_y_train, q1_y_test = train_test_split(\n",
    "    q1_X, q1_y, test_size=0.2, random_state=SEED\n",
    ")\n",
    "\n",
    "# Checks\n",
    "assert q1_X_train.shape[0] + q1_X_test.shape[0] == q1_df.shape[0]\n",
    "assert q1_y_train.shape[0] + q1_y_test.shape[0] == q1_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3754869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shared Preprocessing\n",
    "\n",
    "# Identify columns from predictors only\n",
    "q1_cat_cols = q1_X.select_dtypes(\n",
    "    include=[\"object\", \"category\", \"bool\"]\n",
    ").columns.tolist()\n",
    "q1_num_cols = [c for c in q1_X.columns if c not in q1_cat_cols]\n",
    "\n",
    "# Preprocess: pass numeric through, one-hot encode categoricals\n",
    "q1_preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", q1_num_cols),\n",
    "        (\"cat\", OneHotEncoder(drop=\"if_binary\", handle_unknown=\"ignore\"), q1_cat_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fb0a5",
   "metadata": {},
   "source": [
    "### Q1-Specific Helper Functions and \"Global\" Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665d5f53",
   "metadata": {},
   "source": [
    "### **1(a)** Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Least Squares Linear Model\n",
    "q1a_lm = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", q1_preprocess),\n",
    "        (\"model\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "q1a_lm.fit(q1_X_train, q1_y_train)\n",
    "assert hasattr(q1a_lm.named_steps[\"model\"], \"coef_\"), \"Error in lm fit.\"\n",
    "\n",
    "# Test predictions\n",
    "q1a_y_test_pred = q1a_lm.predict(q1_X_test)\n",
    "\n",
    "# Test MSE\n",
    "q1a_test_mse = mean_squared_error(q1_y_test, q1a_y_test_pred)\n",
    "assert q1a_test_mse > 0, \"MSE greater than zero.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1ca667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test Error Table (LaTex)\n",
    "q1a_mse_tbl = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Test MSE\"],\n",
    "        \"Value\": [q1a_test_mse],\n",
    "    }\n",
    ")\n",
    "\n",
    "q1a_mse_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1a_mse_tbl, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Test-set mean squared error (MSE) for the least squares linear regression model.\",\n",
    "    label=\"tab:q1a-lm-test-mse\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1a_lm_test_mse.tex\").write_text(q1a_mse_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce521ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check from large MSE\n",
    "q1a_test_rmse = np.sqrt(q1a_test_mse)\n",
    "# q1a_test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adadd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract fitted linear model\n",
    "q1a_lr = q1a_lm.named_steps[\"model\"]\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "q1a_feature_names = q1a_lm.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "\n",
    "# Build coefficient table\n",
    "q1a_coef_tbl = pd.DataFrame(\n",
    "    {\n",
    "        \"Predictor\": [\"Intercept\"] + q1a_feature_names.tolist(),\n",
    "        \"Coefficient\": np.concatenate(([q1a_lr.intercept_], q1a_lr.coef_)),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sanity check\n",
    "assert q1a_coef_tbl.shape[0] == len(q1a_lr.coef_) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f28d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Coefficient Table\n",
    "q1a_coef_tbl[\"Predictor\"] = q1a_coef_tbl[\"Predictor\"].str.replace(\n",
    "    \"_\", r\"\\_\", regex=False\n",
    ")\n",
    "\n",
    "q1a_coef_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1a_coef_tbl, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Estimated coefficients for the least squares linear regression model predicting number of applications.\",\n",
    "    label=\"tab:q1a-lm-coefficients\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1a_lm_coefficients.tex\").write_text(q1a_coef_tex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d30ef",
   "metadata": {},
   "source": [
    "### **(b)** Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860808fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Unpruned Regression Tree\n",
    "q1b_tree = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", q1_preprocess),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=SEED)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "q1b_tree.fit(q1_X_train, q1_y_train)\n",
    "assert hasattr(q1b_tree.named_steps[\"model\"], \"tree_\")\n",
    "\n",
    "# Test predictions\n",
    "q1b_y_test_pred = q1b_tree.predict(q1_X_test)\n",
    "\n",
    "# Test MSE\n",
    "q1b_test_mse = mean_squared_error(q1_y_test, q1b_y_test_pred)\n",
    "assert q1b_test_mse > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize Tree Size\n",
    "# Tree summary statistics\n",
    "q1b_model = q1b_tree.named_steps[\"model\"]\n",
    "\n",
    "q1b_depth = q1b_model.get_depth()\n",
    "q1b_n_leaves = q1b_model.get_n_leaves()\n",
    "\n",
    "# q1b_depth, q1b_n_leaves\n",
    "# OUTPUT: (21, np.int64(614))\n",
    "# Too big; don't plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize size in table to justify not plotting\n",
    "q1b_tree_summary_tbl = pd.DataFrame(\n",
    "    {\n",
    "        \"Quantity\": [\"Tree Depth\", \"Number of Terminal Nodes (Leaves)\"],\n",
    "        \"Value\": [q1b_depth, int(q1b_n_leaves)],\n",
    "    }\n",
    ")\n",
    "\n",
    "q1b_tree_summary_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1b_tree_summary_tbl, float_fmt=None, index=False),\n",
    "    caption=\"Summary of the unpruned regression tree fit for predicting applications.\",\n",
    "    label=\"tab:q1b-tree-summary\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1b_tree_summary.tex\").write_text(q1b_tree_summary_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1b_mse_tbl = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Test MSE\"],\n",
    "        \"Value\": [q1b_test_mse],\n",
    "    }\n",
    ")\n",
    "\n",
    "q1b_mse_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1b_mse_tbl, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Test-set mean squared error (MSE) for the unpruned regression tree.\",\n",
    "    label=\"tab:q1b-tree-test-mse\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1b_tree_test_mse.tex\").write_text(q1b_mse_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5941a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check from large MSE\n",
    "q1b_test_rmse = np.sqrt(q1b_test_mse)\n",
    "# q1b_test_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b18db2",
   "metadata": {},
   "source": [
    "### **(c)** Pruning via Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Cost-Complexity Pruning Path\n",
    "\n",
    "# Fit Unpruned Tree on Training Data\n",
    "q1c_tree_unpruned = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", q1_preprocess),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=SEED)),\n",
    "    ]\n",
    ")\n",
    "q1c_tree_unpruned.fit(q1_X_train, q1_y_train)\n",
    "assert hasattr(q1c_tree_unpruned.named_steps[\"model\"], \"tree_\")\n",
    "\n",
    "# Get Transformed Training Matrix to Compute Pruning Path\n",
    "q1c_X_train_trans = q1c_tree_unpruned.named_steps[\"preprocess\"].transform(q1_X_train)\n",
    "\n",
    "# Sanity check\n",
    "assert q1c_X_train_trans.shape[0] == q1_X_train.shape[0]\n",
    "\n",
    "# Compute Pruning Path for the Fitted (Unpruned) Tree\n",
    "q1c_unpruned_model = q1c_tree_unpruned.named_steps[\"model\"]\n",
    "q1c_path = q1c_unpruned_model.cost_complexity_pruning_path(\n",
    "    q1c_X_train_trans, q1_y_train\n",
    ")\n",
    "\n",
    "q1c_ccp_alphas = q1c_path.ccp_alphas\n",
    "q1c_impurities = q1c_path.impurities\n",
    "\n",
    "assert q1c_ccp_alphas.ndim == 1\n",
    "assert len(q1c_ccp_alphas) == len(q1c_impurities)\n",
    "assert (q1c_ccp_alphas >= 0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold CV on Training Data Choose Best alpha\n",
    "q1c_kf = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Drop Last alpha (stump)\n",
    "q1c_alphas = q1c_ccp_alphas[:-1]\n",
    "\n",
    "q1c_cv_rows = []\n",
    "\n",
    "for a in q1c_alphas:\n",
    "    q1c_tree = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", q1_preprocess),\n",
    "            (\"model\", DecisionTreeRegressor(random_state=SEED, ccp_alpha=float(a))),\n",
    "        ]\n",
    "    )\n",
    "    # CV MSE (negative in sklearn)\n",
    "    cv_neg_mse = cross_val_score(\n",
    "        q1c_tree,\n",
    "        q1_X_train,\n",
    "        q1_y_train,\n",
    "        cv=q1c_kf,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "    )\n",
    "    q1c_cv_rows.append(\n",
    "        {\n",
    "            \"ccp_alpha\": float(a),\n",
    "            \"cv_mse_mean\": float((-cv_neg_mse).mean()),\n",
    "            \"cv_mse_std\": float((-cv_neg_mse).std(ddof=1)),\n",
    "        }\n",
    "    )\n",
    "\n",
    "q1c_cv_results = pd.DataFrame(q1c_cv_rows)\n",
    "\n",
    "# Choose alpha with Minimum Mean CV MSE\n",
    "q1c_best_row = q1c_cv_results.loc[q1c_cv_results[\"cv_mse_mean\"].idxmin()]\n",
    "q1c_best_alpha = float(q1c_best_row[\"ccp_alpha\"])\n",
    "\n",
    "# Sanity checks\n",
    "assert q1c_cv_results.shape[0] == len(q1c_alphas)\n",
    "assert q1c_best_alpha >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af52e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Results\n",
    "# print(f\"q1c_best_alpha:  {q1c_best_alpha}\")\n",
    "# print(\"\\nq1c_cv_results:\")\n",
    "# q1c_cv_results.sort_values(\"cv_mse_mean\").head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877c2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV curve figure\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(\n",
    "    q1c_cv_results[\"ccp_alpha\"],\n",
    "    q1c_cv_results[\"cv_mse_mean\"],\n",
    "    marker=\"o\",\n",
    "    markersize=3,\n",
    "    alpha=0.7,\n",
    "    linestyle=\"-\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "\n",
    "# Mark Selected alpha\n",
    "ax.axvline(\n",
    "    q1c_best_alpha,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=2,\n",
    "    label=r\"Selected $\\alpha$\",\n",
    ")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(r\"Cost-Complexity Parameter $\\alpha$\", fontsize=12, labelpad=10)\n",
    "ax.set_ylabel(\"Mean CV MSE\", fontsize=12, labelpad=10)\n",
    "ax.set_title(\n",
    "    \"Cross-Validation Curve for Cost-Complexity Pruning\", fontsize=14, fontweight=\"bold\"\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.5)\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(FIG_DIR / \"q1c_cv_curve.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2705cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Pruned Tree using Best alpha\n",
    "\n",
    "q1c_tree_pruned = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocess\", q1_preprocess),\n",
    "        (\"model\", DecisionTreeRegressor(random_state=SEED, ccp_alpha=q1c_best_alpha)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "q1c_tree_pruned.fit(q1_X_train, q1_y_train)\n",
    "\n",
    "# Sanity check\n",
    "assert hasattr(q1c_tree_pruned.named_steps[\"model\"], \"tree_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece3734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "q1c_y_test_pred = q1c_tree_pruned.predict(q1_X_test)\n",
    "\n",
    "# Test MSE\n",
    "q1c_test_mse = mean_squared_error(q1_y_test, q1c_y_test_pred)\n",
    "assert q1c_test_mse > 0\n",
    "\n",
    "# RMSE sanity check\n",
    "q1c_test_rmse = float(np.sqrt(q1c_test_mse))\n",
    "# q1c_test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d818958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Pruned Depth and Num Leaves\n",
    "q1c_pruned_model = q1c_tree_pruned.named_steps[\"model\"]\n",
    "\n",
    "q1c_pruned_depth = q1c_pruned_model.get_depth()\n",
    "q1c_pruned_n_leaves = q1c_pruned_model.get_n_leaves()\n",
    "\n",
    "assert q1c_pruned_depth >= 0\n",
    "assert q1c_pruned_n_leaves >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310f09f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Pruned Test MSE to LaTex table\n",
    "q1c_pruned_mse_tbl = pd.DataFrame({\"Metric\": [\"Test MSE\"], \"Value\": [q1c_test_mse]})\n",
    "\n",
    "q1c_pruned_mse_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1c_pruned_mse_tbl, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Test-set mean squared error (MSE) for the pruned regression tree.\",\n",
    "    label=\"tab:q1c-pruned-tree-test-mse\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1c_pruned_tree_test_mse.tex\").write_text(q1c_pruned_mse_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison Table: Unpruned vs Pruned Trees\n",
    "q1c_tree_comparison_tbl = pd.DataFrame(\n",
    "    {\n",
    "        \"Quantity\": [\n",
    "            \"Tree depth\",\n",
    "            \"Number of terminal nodes (leaves)\",\n",
    "            \"Test MSE\",\n",
    "        ],\n",
    "        \"Unpruned Tree\": [\n",
    "            q1b_depth,\n",
    "            int(q1b_n_leaves),\n",
    "            q1b_test_mse,\n",
    "        ],\n",
    "        \"Pruned Tree\": [\n",
    "            q1c_pruned_depth,\n",
    "            int(q1c_pruned_n_leaves),\n",
    "            q1c_test_mse,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "q1c_tree_comparison_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1c_tree_comparison_tbl, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Comparison of unpruned and pruned regression trees for predicting applications.\",\n",
    "    label=\"tab:q1c-tree-comparison\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1c_tree_comparison.tex\").write_text(q1c_tree_comparison_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f115f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Importance\n",
    "q1c_feature_names = q1c_tree_pruned.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "q1c_importances = q1c_tree_pruned.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# Sanity checks\n",
    "assert len(q1c_feature_names) == len(q1c_importances)\n",
    "assert np.isclose(\n",
    "    q1c_importances.sum(), 1.0\n",
    "), f\"Feature importances sum to {q1c_importances.sum():.6f}, expected 1.0\"\n",
    "assert (q1c_importances >= 0).all()\n",
    "\n",
    "q1c_importance_df = pd.DataFrame(\n",
    "    {\"Predictor\": q1c_feature_names, \"Importance\": q1c_importances}\n",
    ").sort_values(\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ad43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Importance Table to LaTex\n",
    "q1c_importance_full = q1c_importance_df.copy()\n",
    "\n",
    "# Escape underscores for LaTeX\n",
    "q1c_importance_full[\"Predictor\"] = q1c_importance_full[\"Predictor\"].str.replace(\n",
    "    \"_\", r\"\\_\", regex=False\n",
    ")\n",
    "\n",
    "q1c_importance_tex = wrap_table(\n",
    "    df_to_tabular_tex(q1c_importance_full, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Variable importances from the pruned regression tree. Predictors with zero importance were not used in any split.\",\n",
    "    label=\"tab:q1c-pruned-tree-importance\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q1c_pruned_tree_importance.tex\").write_text(q1c_importance_tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed44c3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruned Tree Plot: Minimal-labels (feature + threshold only)\n",
    "q1c_pruned_model = q1c_tree_pruned.named_steps[\"model\"]\n",
    "q1c_feature_names = q1c_tree_pruned.named_steps[\"preprocess\"].get_feature_names_out()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 10))\n",
    "\n",
    "plot_tree(\n",
    "    q1c_pruned_model,\n",
    "    feature_names=q1c_feature_names,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    impurity=False,\n",
    "    node_ids=False,\n",
    "    proportion=False,   # no sample proportions\n",
    "    label=\"root\",       # fewer labels overall\n",
    "    fontsize=8,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "ax.set_title(\"Pruned Regression Tree\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig.savefig(FIG_DIR / \"q1c_pruned_tree.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39da288a",
   "metadata": {},
   "source": [
    "### **(d)** Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2618dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dabe9af",
   "metadata": {},
   "source": [
    "### **(e)** Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ec5512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b88c0cc9",
   "metadata": {},
   "source": [
    "### **(f)** Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd49529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c1e906",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12344e1a",
   "metadata": {},
   "source": [
    "## **Question 2:** Business School Admissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a567f",
   "metadata": {},
   "source": [
    "### Data Load, Initial Inspection, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "q2_df = pd.read_csv(DATA_DIR / \"admission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982118c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial EDA summary\n",
    "de_levels = sorted(q2_df[\"De\"].astype(str).unique().tolist())\n",
    "group_levels = sorted(q2_df[\"Group\"].unique().tolist())\n",
    "\n",
    "class_counts = q2_df[\"Group\"].value_counts().sort_index()\n",
    "\n",
    "eda_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"item\": [\n",
    "            \"n_rows\",\n",
    "            \"n_cols\",\n",
    "            \"columns\",\n",
    "            \"De_levels\",\n",
    "            \"Group_levels\",\n",
    "            \"Group_counts\",\n",
    "        ],\n",
    "        \"value\": [\n",
    "            q2_df.shape[0],\n",
    "            q2_df.shape[1],\n",
    "            \", \".join(q2_df.columns.astype(str).tolist()),\n",
    "            \", \".join(de_levels),\n",
    "            \", \".join(map(str, group_levels)),\n",
    "            \"; \".join([f\"{k}:{v}\" for k, v in class_counts.items()]),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "# eda_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18584be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Verify categorical labels match numeric group coding\n",
    "category_map = {\"admit\": 1, \"notadmit\": 2, \"border\": 3}\n",
    "\n",
    "categories_match = (q2_df[\"De\"].map(category_map) == q2_df[\"Group\"]).all()\n",
    "assert categories_match, \"Mismatch between De labels and Group codes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split:\n",
    "## Last 4 observations in each category -> test\n",
    "## Rest of observations -> train\n",
    "\n",
    "q2_test_idx = q2_df.groupby(\"Group\", sort=False).tail(4).index\n",
    "\n",
    "\n",
    "q2_train_df = q2_df.drop(index=q2_test_idx).copy()\n",
    "q2_test_df = q2_df.loc[q2_test_idx].copy()\n",
    "\n",
    "# Features/labels\n",
    "q2_X_train = q2_train_df[[\"GPA\", \"GMAT\"]].to_numpy()\n",
    "q2_y_train = q2_train_df[\"Group\"].to_numpy()\n",
    "\n",
    "q2_X_test = q2_test_df[[\"GPA\", \"GMAT\"]].to_numpy()\n",
    "q2_y_test = q2_test_df[\"Group\"].to_numpy()\n",
    "\n",
    "# Checks\n",
    "assert len(q2_test_df) == 12\n",
    "assert q2_train_df.shape[0] + q2_test_df.shape[0] == q2_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c96b411",
   "metadata": {},
   "source": [
    "### Q2-Specific Helper Functions and \"Global\" Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for scatter plots\n",
    "def plot_q2_scatter(df, ax, order, labels, colors, title=None, alpha=0.7):\n",
    "    \"\"\"Scatter plot of GPA vs GMAT colored by admission group.\"\"\"\n",
    "    for g in order:\n",
    "        subset = df[df[\"Group\"] == g]\n",
    "        ax.scatter(\n",
    "            subset[\"GPA\"],\n",
    "            subset[\"GMAT\"],\n",
    "            label=labels[g],\n",
    "            color=colors[g],\n",
    "            alpha=alpha,\n",
    "            edgecolor=\"k\",\n",
    "            s=60,\n",
    "        )\n",
    "    ax.set_xlabel(\"Undergraduate GPA\", fontsize=12, labelpad=10)\n",
    "    ax.set_ylabel(\"GMAT Score\", fontsize=12, labelpad=10)\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# Mesh for plotting\n",
    "def build_gpa_gmat_mesh(df, *, gpa_pad=0.05, gmat_pad=10, n=300):\n",
    "    gpa_min, gpa_max = df[\"GPA\"].min() - gpa_pad, df[\"GPA\"].max() + gpa_pad\n",
    "    gmat_min, gmat_max = df[\"GMAT\"].min() - gmat_pad, df[\"GMAT\"].max() + gmat_pad\n",
    "\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(gpa_min, gpa_max, n),\n",
    "        np.linspace(gmat_min, gmat_max, n),\n",
    "    )\n",
    "\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    return xx, yy, grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c8718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Color/label mapping for groups\n",
    "q2_labels = {1: \"Admit\", 2: \"Not Admit\", 3: \"Border\"}\n",
    "q2_colors = {1: \"tab:blue\", 2: \"tab:orange\", 3: \"tab:green\"}\n",
    "q2_order = [2, 3, 1]\n",
    "\n",
    "# Names in order: Not Admit, Border, Admit\n",
    "q2_group_names = [q2_labels[g] for g in q2_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86a0455",
   "metadata": {},
   "source": [
    "### **2(a)** Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820d253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: GPA vs GMAT, colored by groups\n",
    "fig_scatter, ax_scatter = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "plot_q2_scatter(\n",
    "    df=q2_train_df,\n",
    "    ax=ax_scatter,\n",
    "    order=q2_order,\n",
    "    labels=q2_labels,\n",
    "    colors=q2_colors,\n",
    "    title=\"Training Data: GPA vs GMAT by Admission Group\",\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "fig_scatter.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "fig_scatter.savefig(FIG_DIR / \"q2_scatter_gpa_gmat.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c72905",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_train_df_plot = q2_train_df.copy()\n",
    "q2_train_df_plot[\"GroupOrdered\"] = pd.Categorical(\n",
    "    q2_train_df_plot[\"Group\"].map(q2_labels),\n",
    "    categories=[q2_labels[g] for g in q2_order],\n",
    "    ordered=True,\n",
    ")\n",
    "\n",
    "fig_box, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Boxplot: GPA by Group\n",
    "q2_train_df_plot.boxplot(column=\"GPA\", by=\"GroupOrdered\", ax=axes[0], grid=False)\n",
    "axes[0].set_title(\"GPA by Admission Group\", fontsize=13)\n",
    "axes[0].set_xlabel(\"Group\", fontsize=12, labelpad=10)\n",
    "axes[0].set_ylabel(\"GPA\", fontsize=12, labelpad=10)\n",
    "\n",
    "# Boxplot: GMAT by Group\n",
    "q2_train_df_plot.boxplot(column=\"GMAT\", by=\"GroupOrdered\", ax=axes[1], grid=False)\n",
    "axes[1].set_title(\"GMAT by Admission Group\", fontsize=13)\n",
    "axes[1].set_xlabel(\"Group\", fontsize=12, labelpad=10)\n",
    "axes[1].set_ylabel(\"GMAT Score\", fontsize=12, labelpad=10)\n",
    "\n",
    "fig_box.suptitle(\n",
    "    \"Training Data: Marginal Distributions by Group\", fontsize=16, fontweight=\"bold\"\n",
    ")\n",
    "fig_box.tight_layout()\n",
    "\n",
    "\n",
    "# Save figure\n",
    "fig_box.savefig(FIG_DIR / \"q2_boxplots_gpa_gmat.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad85d72",
   "metadata": {},
   "source": [
    "### **2(b)** LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591d678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LDA on training data\n",
    "q2_lda = LinearDiscriminantAnalysis()\n",
    "q2_lda.fit(q2_X_train, q2_y_train)\n",
    "\n",
    "# Predictions\n",
    "q2_lda_train_pred = q2_lda.predict(q2_X_train)\n",
    "q2_lda_test_pred = q2_lda.predict(q2_X_test)\n",
    "\n",
    "# Sanity checks\n",
    "assert len(q2_lda_train_pred) == len(q2_y_train)\n",
    "assert len(q2_lda_test_pred) == len(q2_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ce27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mesh over predictor space\n",
    "xx, yy, grid = build_gpa_gmat_mesh(q2_train_df)\n",
    "Z = q2_lda.predict(grid).reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions + training points\n",
    "fig_lda, ax_lda = plt.subplots(figsize=(6.5, 5.5))\n",
    "\n",
    "# Decision regions\n",
    "ax_lda.contourf(xx, yy, Z, alpha=0.18)\n",
    "\n",
    "# Decision boundaries\n",
    "ax_lda.contour(xx, yy, Z, levels=[1.5, 2.5], colors=\"k\", linewidths=1)\n",
    "\n",
    "plot_q2_scatter(\n",
    "    df=q2_train_df,\n",
    "    ax=ax_lda,\n",
    "    order=q2_order,\n",
    "    labels=q2_labels,\n",
    "    colors=q2_colors,\n",
    "    title=\"LDA Decision Regions (Training Data)\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "fig_lda.tight_layout()\n",
    "\n",
    "# Save plot artifact\n",
    "fig_lda.savefig(FIG_DIR / \"q2_lda_boundary.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5332cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "# Rows = true class, Columns = predicted class\n",
    "# Order: Not Admit (2), Border (3), Admit (1)\n",
    "q2_lda_cm_train = confusion_matrix(q2_y_train, q2_lda_train_pred, labels=q2_order)\n",
    "\n",
    "q2_lda_cm_test = confusion_matrix(q2_y_test, q2_lda_test_pred, labels=q2_order)\n",
    "\n",
    "# Overall misclassification rates\n",
    "q2_lda_train_err = 1.0 - np.mean(q2_lda_train_pred == q2_y_train)\n",
    "q2_lda_test_err = 1.0 - np.mean(q2_lda_test_pred == q2_y_test)\n",
    "\n",
    "# Sanity checks\n",
    "assert q2_lda_cm_train.shape == (3, 3)\n",
    "assert q2_lda_cm_test.shape == (3, 3)\n",
    "assert 0.0 <= q2_lda_train_err <= 1.0\n",
    "assert 0.0 <= q2_lda_test_err <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253053aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices as labeled DataFrames\n",
    "q2_lda_cm_train_df = pd.DataFrame(\n",
    "    q2_lda_cm_train, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "q2_lda_cm_test_df = pd.DataFrame(\n",
    "    q2_lda_cm_test, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "\n",
    "# Error-rate summary\n",
    "q2_lda_metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Misclassification Rate\": [q2_lda_train_err, q2_lda_test_err],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create LaTex tables\n",
    "q2_lda_all_tables_tex = \"\\n\".join(\n",
    "    [\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_lda_cm_train_df, index=True, float_fmt=None),\n",
    "            caption=\"LDA confusion matrix (training data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_lda_cm_train\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_lda_cm_test_df, index=True, float_fmt=None),\n",
    "            caption=\"LDA confusion matrix (test data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_lda_cm_test\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_lda_metrics_df, index=False, float_fmt=\"%.4f\"),\n",
    "            caption=\"LDA overall misclassification rates.\",\n",
    "            label=\"tab:q2_lda_error_rates\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write output\n",
    "(TAB_DIR / \"q2_lda_tables.tex\").write_text(q2_lda_all_tables_tex, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429e5efe",
   "metadata": {},
   "source": [
    "\n",
    "### **2(c)** QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928fb507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit QDA on training data\n",
    "q2_qda = QuadraticDiscriminantAnalysis()\n",
    "q2_qda.fit(q2_X_train, q2_y_train)\n",
    "\n",
    "# Predictions\n",
    "q2_qda_train_pred = q2_qda.predict(q2_X_train)\n",
    "q2_qda_test_pred = q2_qda.predict(q2_X_test)\n",
    "\n",
    "# Sanity checks\n",
    "assert len(q2_qda_train_pred) == len(q2_y_train)\n",
    "assert len(q2_qda_test_pred) == len(q2_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecc5a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mesh over predictor space\n",
    "xx, yy, grid = build_gpa_gmat_mesh(q2_train_df)\n",
    "Z = q2_qda.predict(grid).reshape(xx.shape)\n",
    "\n",
    "# Plot decision regions + training points\n",
    "fig_qda, ax_qda = plt.subplots(figsize=(6.5, 5.5))\n",
    "\n",
    "# Decision regions\n",
    "ax_qda.contourf(xx, yy, Z, alpha=0.18)\n",
    "\n",
    "# Decision boundaries\n",
    "ax_qda.contour(xx, yy, Z, levels=[1.5, 2.5], colors=\"k\", linewidths=1)\n",
    "\n",
    "plot_q2_scatter(\n",
    "    df=q2_train_df,\n",
    "    ax=ax_qda,\n",
    "    order=q2_order,\n",
    "    labels=q2_labels,\n",
    "    colors=q2_colors,\n",
    "    title=\"QDA Decision Regions (Training Data)\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "fig_qda.tight_layout()\n",
    "fig_qda.savefig(FIG_DIR / \"q2_qda_boundary.png\", bbox_inches=\"tight\")\n",
    "plt.close(fig_qda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503daf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "# Rows = true class, Columns = predicted class\n",
    "# Order: Not Admit (2), Border (3), Admit (1)\n",
    "q2_qda_cm_train = confusion_matrix(q2_y_train, q2_qda_train_pred, labels=q2_order)\n",
    "\n",
    "q2_qda_cm_test = confusion_matrix(q2_y_test, q2_qda_test_pred, labels=q2_order)\n",
    "\n",
    "# Misclassification rates\n",
    "q2_qda_train_err = 1.0 - np.mean(q2_qda_train_pred == q2_y_train)\n",
    "q2_qda_test_err = 1.0 - np.mean(q2_qda_test_pred == q2_y_test)\n",
    "\n",
    "# Sanity checks\n",
    "assert q2_qda_cm_train.shape == (3, 3)\n",
    "assert q2_qda_cm_test.shape == (3, 3)\n",
    "assert 0.0 <= q2_qda_train_err <= 1.0\n",
    "assert 0.0 <= q2_qda_test_err <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2488e0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices as labeled DataFrames\n",
    "q2_qda_cm_train_df = pd.DataFrame(\n",
    "    q2_qda_cm_train, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "q2_qda_cm_test_df = pd.DataFrame(\n",
    "    q2_qda_cm_test, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "\n",
    "# Error-rate summary\n",
    "q2_qda_metrics_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Misclassification Rate\": [q2_qda_train_err, q2_qda_test_err],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create LaTex tables\n",
    "qda_tables_tex = \"\\n\".join(\n",
    "    [\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_qda_cm_train_df, float_fmt=None),\n",
    "            caption=\"QDA confusion matrix (training data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_qda_cm_train\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_qda_cm_test_df, float_fmt=None),\n",
    "            caption=\"QDA confusion matrix (test data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_qda_cm_test\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_qda_metrics_df, index=False),\n",
    "            caption=\"QDA overall misclassification rates.\",\n",
    "            label=\"tab:q2_qda_error_rates\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write output\n",
    "(TAB_DIR / \"q2_qda_tables.tex\").write_text(qda_tables_tex, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4901f473",
   "metadata": {},
   "source": [
    "### **2(d)** KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4246a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose K by minimizing Test Misclassification Rate\n",
    "\n",
    "# Odd k to Reduce class tie frequencies\n",
    "q2_knn_K_values = list(range(1, 26, 2))\n",
    "q2_knn_test_errors = []\n",
    "\n",
    "for K in q2_knn_K_values:\n",
    "    # Use Pipeline to avoid scaling data leakage and scaling errors\n",
    "    q2_knn = Pipeline(\n",
    "        [(\"scalar\", StandardScaler()), (\"knn\", KNeighborsClassifier(n_neighbors=K))]\n",
    "    )\n",
    "    q2_knn.fit(q2_X_train, q2_y_train)\n",
    "    q2_test_pred = q2_knn.predict(q2_X_test)\n",
    "\n",
    "    test_err = 1.0 - np.mean(q2_test_pred == q2_y_test)\n",
    "    q2_knn_test_errors.append(test_err)\n",
    "\n",
    "q2_knn_results_df = pd.DataFrame(\n",
    "    {\"K\": q2_knn_K_values, \"test_error_rate\": q2_knn_test_errors}\n",
    ")\n",
    "\n",
    "# Optimal K (on test error ties, use smallest K)\n",
    "q2_knn_results_sorted = q2_knn_results_df.sort_values(\n",
    "    [\"test_error_rate\", \"K\"], ascending=[True, True]\n",
    ")\n",
    "\n",
    "q2_knn_best_row = q2_knn_results_sorted.iloc[0]\n",
    "\n",
    "q2_knn_K_star = int(q2_knn_best_row[\"K\"])\n",
    "q2_knn_test_err_star = float(q2_knn_best_row[\"test_error_rate\"])\n",
    "\n",
    "# Sanity Checks\n",
    "assert q2_knn_K_star in q2_knn_K_values\n",
    "assert 0.0 <= q2_knn_test_err_star <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3912e433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save K-sweep results to LaTex table\n",
    "q2_knn_results_sorted_rename = q2_knn_results_sorted.rename(\n",
    "    columns={\"test_error_rate\": \"Test Error Rate\"}\n",
    ")\n",
    "\n",
    "q2_knn_sweep_tex = wrap_table(\n",
    "    df_to_tabular_tex(q2_knn_results_sorted_rename, index=False),\n",
    "    caption=\"Sorted KNN test error rate (three-class) for candidate values of $K$.\",\n",
    "    label=\"tab:q2_knn_k_sweep\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q2_knn_k_sweep.tex\").write_text(q2_knn_sweep_tex, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0716ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit K* model on full 3-class training data\n",
    "q2_knn_star = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsClassifier(n_neighbors=q2_knn_K_star)),\n",
    "    ]\n",
    ")\n",
    "q2_knn_star.fit(q2_X_train, q2_y_train)\n",
    "\n",
    "# 3-class predictions\n",
    "q2_knn_train_pred3 = q2_knn_star.predict(q2_X_train)\n",
    "q2_knn_test_pred3 = q2_knn_star.predict(q2_X_test)\n",
    "\n",
    "# 3-class confusion matrices (ordered as Not Admit, Border, Admit)\n",
    "q2_knn_cm_train3 = confusion_matrix(q2_y_train, q2_knn_train_pred3, labels=q2_order)\n",
    "q2_knn_cm_test3 = confusion_matrix(q2_y_test, q2_knn_test_pred3, labels=q2_order)\n",
    "\n",
    "# 3-class misclassification rates\n",
    "q2_knn_train_err3 = 1.0 - np.mean(q2_knn_train_pred3 == q2_y_train)\n",
    "q2_knn_test_err3 = 1.0 - np.mean(q2_knn_test_pred3 == q2_y_test)\n",
    "\n",
    "# Sanity Checks\n",
    "assert q2_knn_cm_train3.shape == (3, 3)\n",
    "assert q2_knn_cm_test3.shape == (3, 3)\n",
    "assert 0.0 <= q2_knn_train_err3 <= 1.0\n",
    "assert 0.0 <= q2_knn_test_err3 <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices as labeled DataFrames\n",
    "q2_knn_cm_train3_df = pd.DataFrame(\n",
    "    q2_knn_cm_train3, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "q2_knn_cm_test3_df = pd.DataFrame(\n",
    "    q2_knn_cm_test3, index=q2_group_names, columns=q2_group_names\n",
    ")\n",
    "\n",
    "# Error-rate summary\n",
    "q2_knn_err3_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Dataset\": [\"Training\", \"Test\"],\n",
    "        \"Misclassification Rate\": [q2_knn_train_err3, q2_knn_test_err3],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create LaTex tables\n",
    "q2_knn_tables3_tex = \"\\n\".join(\n",
    "    [\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_knn_cm_train3_df, float_fmt=None),\n",
    "            caption=\"KNN confusion matrix at $K^*$ (training data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_knn_cm_train3\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_knn_cm_test3_df, float_fmt=None),\n",
    "            caption=\"KNN confusion matrix at $K^*$ (test data). Rows are true classes and columns are predicted classes.\",\n",
    "            label=\"tab:q2_knn_cm_test3\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_knn_err3_df, index=False),\n",
    "            caption=\"KNN misclassification rates (three-class) at $K^*$.\",\n",
    "            label=\"tab:q2_knn_err3\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Write output\n",
    "(TAB_DIR / \"q2_knn_3class_tables.tex\").write_text(q2_knn_tables3_tex, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c301d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass OvR metrics on training data for KNN at K*\n",
    "\n",
    "# Confusion Matrix Totals\n",
    "n = q2_knn_cm_train3.sum()\n",
    "rowsum = q2_knn_cm_train3.sum(axis=1)  # actual totals per class\n",
    "colsum = q2_knn_cm_train3.sum(axis=0)  # predicted totals per class\n",
    "diag = np.diag(q2_knn_cm_train3)\n",
    "\n",
    "# OvR per-class sensitivity and specificity\n",
    "sens = diag / rowsum\n",
    "spec = (n - rowsum - colsum + diag) / (n - rowsum)\n",
    "\n",
    "# Macro averages\n",
    "sens_macro = sens.mean()\n",
    "spec_macro = spec.mean()\n",
    "\n",
    "# Multiclass AUC (OvR macro) using predicted probabilities\n",
    "proba_train = q2_knn_star.predict_proba(q2_X_train)\n",
    "\n",
    "# Align columns to labels [1,2,3] for `roc_auc_score()`\n",
    "proba_df = pd.DataFrame(proba_train, columns=q2_knn_star.named_steps[\"knn\"].classes_)\n",
    "proba_aligned = proba_df[[1, 2, 3]].to_numpy()\n",
    "\n",
    "q2_knn_auc_ovr_macro = roc_auc_score(\n",
    "    q2_y_train, proba_aligned, multi_class=\"ovr\", average=\"macro\"\n",
    ")\n",
    "\n",
    "# Package for table\n",
    "q2_knn_multiclass_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Class\": q2_group_names,\n",
    "        \"Sensitivity (OvR)\": sens,\n",
    "        \"Specificity (OvR)\": spec,\n",
    "    }\n",
    ")\n",
    "\n",
    "q2_knn_multiclass_summary_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Metric\": [\"Macro sensitivity\", \"Macro specificity\", \"AUC (OvR macro)\"],\n",
    "        \"Value\": [sens_macro, spec_macro, q2_knn_auc_ovr_macro],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Sanity checks\n",
    "assert q2_knn_multiclass_df.shape == (3, 3)\n",
    "assert 0.0 <= sens_macro <= 1.0\n",
    "assert 0.0 <= spec_macro <= 1.0\n",
    "assert 0.0 <= q2_knn_auc_ovr_macro <= 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_knn_multiclass_tex = \"\\n\".join(\n",
    "    [\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(q2_knn_multiclass_df, float_fmt=\"%.4f\", index=False),\n",
    "            caption=\"KNN multiclass one-vs-rest (OvR) sensitivity and specificity by class (training data) at $K^*$.\",\n",
    "            label=\"tab:q2_knn_ovr_by_class\",\n",
    "        ),\n",
    "        wrap_table(\n",
    "            df_to_tabular_tex(\n",
    "                q2_knn_multiclass_summary_df, float_fmt=\"%.4f\", index=False\n",
    "            ),\n",
    "            caption=\"KNN multiclass OvR macro-averaged metrics (training data) at $K^*$.\",\n",
    "            label=\"tab:q2_knn_ovr_summary\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q2_knn_multiclass_metrics.tex\").write_text(\n",
    "    q2_knn_multiclass_tex, encoding=\"utf-8\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8447ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final KNN summary table\n",
    "\n",
    "q2_knn_summary_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Quantity\": [\n",
    "            r\"$K^*$\",\n",
    "            \"Training error rate\",\n",
    "            \"Training sensitivity (OvR, macro)\",\n",
    "            \"Training specificity (OvR, macro)\",\n",
    "            \"Training AUC (OvR, macro)\",\n",
    "            \"Test error rate\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            q2_knn_K_star,\n",
    "            q2_knn_train_err3,\n",
    "            sens_macro,\n",
    "            spec_macro,\n",
    "            q2_knn_auc_ovr_macro,\n",
    "            q2_knn_test_err3,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "q2_knn_summary_tex = wrap_table(\n",
    "    df_to_tabular_tex(q2_knn_summary_df, float_fmt=\"%.4f\", index=False),\n",
    "    caption=\"Summary of optimal KNN performance.\",\n",
    "    label=\"tab:q2_knn_summary\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q2_knn_summary.tex\").write_text(q2_knn_summary_tex, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2a510",
   "metadata": {},
   "source": [
    "### **2(e)** Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb077798",
   "metadata": {},
   "outputs": [],
   "source": [
    "q2_model_comparison_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Model\": [\"LDA\", \"QDA\", \"KNN (K=3)\"],\n",
    "        \"Training Error Rate\": [\n",
    "            q2_lda_train_err,\n",
    "            q2_qda_train_err,\n",
    "            q2_knn_train_err3,\n",
    "        ],\n",
    "        \"Test Error Rate\": [\n",
    "            q2_lda_test_err,\n",
    "            q2_qda_test_err,\n",
    "            q2_knn_test_err3,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "\n",
    "q2_model_comparison_tex = wrap_table(\n",
    "    df_to_tabular_tex(q2_model_comparison_df, index=False),\n",
    "    caption=\"Comparison of training and test misclassification rates for LDA, QDA, and KNN.\",\n",
    "    label=\"tab:q2_model_comparison\",\n",
    ")\n",
    "\n",
    "(TAB_DIR / \"q2_model_comparison.tex\").write_text(\n",
    "    q2_model_comparison_tex, encoding=\"utf-8\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
